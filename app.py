"""
RAGAS í‰ê°€ ì‹œìŠ¤í…œ ì›¹ UI
Streamlitì„ ì‚¬ìš©í•œ RAG ì‹œìŠ¤í…œ í‰ê°€ ìë™í™” ë„êµ¬
"""

import json
import os
import shutil
import sys
import tempfile
import uuid
import zipfile
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple

import pandas as pd
import streamlit as st
from datasets import Dataset

# ë¡œì»¬ ëª¨ë“ˆ import
from askToLangflow import ask_to_langflow, load_payload, save_payload
from ragasEvaluator import (
    load_ragas_data,
    prepare_evaluation_data,
    run_ragas_evaluation,
    save_evaluation_results,
)
from responseParser import parse_response, save_parsed_response

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="RAGAS í‰ê°€ ì‹œìŠ¤í…œ",
    page_icon="ğŸ“Š",
    layout="wide"
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'evaluation_results' not in st.session_state:
    st.session_state.evaluation_results = []
if 'is_running' not in st.session_state:
    st.session_state.is_running = False
if 'progress' not in st.session_state:
    st.session_state.progress = 0
if 'total_queries' not in st.session_state:
    st.session_state.total_queries = 0
if 'zip_files_to_cleanup' not in st.session_state:
    st.session_state.zip_files_to_cleanup = []

# ì´ì „ ì„¸ì…˜ì˜ ZIP íŒŒì¼ ì •ë¦¬ (ì˜µì…˜)
# ì£¼ì˜: ì´ì „ ì„¸ì…˜ì˜ ZIP íŒŒì¼ì„ ì¦‰ì‹œ ì‚­ì œí•˜ë©´ ë‹¤ìš´ë¡œë“œ ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ
# ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œëŠ” ë” ì •êµí•œ ì •ë¦¬ ì „ëµ í•„ìš” (ì˜ˆ: íƒ€ì„ì•„ì›ƒ ê¸°ë°˜)


def collect_query_data(
    query_data: Dict,
    langflow_url: str,
    langflow_api_key: str,
    temp_dir: Path
) -> Dict:
    """
    ë‹¨ì¼ ì¿¼ë¦¬ì— ëŒ€í•´ ìš”ì²­ì„ ë³´ë‚´ê³  ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    (í‰ê°€ëŠ” í•˜ì§€ ì•ŠìŒ)
    
    Args:
        query_data: ì¿¼ë¦¬ ë°ì´í„° (query, ground_truth, gt_context)
        langflow_url: Langflow API URL
        langflow_api_key: Langflow API Key
        temp_dir: ì„ì‹œ ë””ë ‰í† ë¦¬
    
    Returns:
        ìˆ˜ì§‘ëœ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ (parsed_data í¬í•¨)
    """
    try:
        # 1. í™˜ê²½ë³€ìˆ˜ ì„¤ì •
        os.environ['LANGFLOW_URL'] = langflow_url
        os.environ['LANGFLOW_API_KEY'] = langflow_api_key
        
        # 2. Payload ìƒì„± (ì €ì¥í•˜ì§€ ì•ŠìŒ)
        payload = {
            "input_value": query_data["query"],
            "output_type": "chat",
            "input_type": "chat"
        }
        
        # 3. Langflow API ìš”ì²­
        # ask_to_langflow í˜¸ì¶œ (ìë™ìœ¼ë¡œ data/responsesì— ì €ì¥ë˜ê³  ì €ì¥ ê²½ë¡œ ë°˜í™˜)
        response_data, saved_response_filepath = ask_to_langflow(payload, url=langflow_url, return_filepath=True)
        
        # ì €ì¥ëœ ì‘ë‹µ íŒŒì¼ ì½ê¸° (ì„ì‹œ ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬í•˜ì§€ ì•Šê³  ì§ì ‘ ì½ê¸°)
        saved_response_path = Path(saved_response_filepath)
        if saved_response_path.exists():
            # ì €ì¥ëœ ì „ì²´ êµ¬ì¡° ì½ê¸° (parse_responseì— ì „ë‹¬í•˜ê¸° ìœ„í•´)
            with open(saved_response_path, 'r', encoding='utf-8') as f:
                saved_response_data = json.load(f)
        else:
            # íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° response_dataë¥¼ ì‚¬ìš© (fallback)
            saved_response_data = {
                'response': response_data
            }
        
        # 4. ì‘ë‹µ íŒŒì‹±
        # saved_response_dataëŠ” {'timestamp': ..., 'url': ..., 'payload': ..., 'response': ...} êµ¬ì¡°
        parsed_data = parse_response(saved_response_data)
        
        # 5. íŒŒì‹±ëœ ì‘ë‹µ ì €ì¥
        parsed_path = temp_dir / "parsed_responses" / f"parsed_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}_{uuid.uuid4().hex[:8]}.json"
        parsed_path.parent.mkdir(parents=True, exist_ok=True)
        save_parsed_response(parsed_data, str(parsed_path))
        
        return {
            'query': query_data["query"],
            'success': True,
            'query_data': query_data,
            'parsed_data': parsed_data,
            'parsed_path': str(parsed_path),
            'response_path': str(saved_response_path)  # ì˜êµ¬ ì €ì¥ëœ ì‘ë‹µ íŒŒì¼ ê²½ë¡œ ì¶”ê°€
        }
        
    except Exception as e:
        return {
            'query': query_data["query"],
            'success': False,
            'error': str(e),
            'error_type': type(e).__name__
        }


def prepare_batch_evaluation_data(
    collected_data: List[Dict]
) -> Tuple[Dataset, List[Dict]]:
    """
    ìˆ˜ì§‘ëœ ëª¨ë“  ë°ì´í„°ë¥¼ RAGAS í‰ê°€ìš© Datasetìœ¼ë¡œ ì¤€ë¹„í•©ë‹ˆë‹¤.
    ë‹µë³€ì´ ì—†ê±°ë‚˜ contextê°€ nullì¸ ì¼€ì´ìŠ¤ëŠ” ì œì™¸í•©ë‹ˆë‹¤.
    
    Args:
        collected_data: ìˆ˜ì§‘ëœ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ (ê° í•­ëª©ì€ query_dataì™€ parsed_data í¬í•¨)
    
    Returns:
        tuple: (Dataset, í‰ê°€ì— í¬í•¨ëœ ë°ì´í„° ë¦¬ìŠ¤íŠ¸)
    """
    questions = []
    answers = []
    contexts = []
    ground_truths = []
    ground_truths_list = []
    included_items = []  # í‰ê°€ì— í¬í•¨ëœ í•­ëª©ë“¤
    
    for item in collected_data:
        if not item['success']:
            continue
        
        parsed_data = item.get('parsed_data', {})
        answer = parsed_data.get('answer', '')
        context = parsed_data.get('context', '')
        
        # í‰ê°€ì—ì„œ ì œì™¸í•  ì¼€ì´ìŠ¤ ì²´í¬
        # 1. ë‹µë³€ì´ ì—†ëŠ” ê²½ìš°
        if not answer or answer.strip() == '':
            continue
        
        # 2. ë‹µë³€ì€ ìˆì§€ë§Œ contextê°€ nullì´ê±°ë‚˜ ë¹ˆ ë¬¸ìì—´ì¸ ê²½ìš°
        if not context or context.strip() == '':
            continue
        
        # í‰ê°€ì— í¬í•¨
        questions.append(item['query_data']['query'])
        answers.append(answer)
        contexts.append([context])
        ground_truths.append(item['query_data']['ground_truth'])
        ground_truths_list.append([item['query_data']['gt_context']])
        included_items.append(item)
    
    if len(questions) == 0:
        raise ValueError("í‰ê°€í•  ìˆ˜ ìˆëŠ” ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (ë‹µë³€ì´ ì—†ê±°ë‚˜ contextê°€ nullì¸ ì¼€ì´ìŠ¤ëŠ” ì œì™¸ë©ë‹ˆë‹¤)")
    
    data_dict = {
        "question": questions,
        "answer": answers,
        "contexts": contexts,
        "ground_truth": ground_truths,
        "ground_truths": ground_truths_list,
    }
    
    from datasets import Dataset
    dataset = Dataset.from_dict(data_dict)
    return dataset, included_items


def create_zip_file(results_dir: Path, output_path: Path, collected_data: List[Dict] = None):
    """
    ê²°ê³¼ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  íŒŒì¼ì„ ZIP íŒŒì¼ë¡œ ì••ì¶•í•©ë‹ˆë‹¤.
    ì˜êµ¬ ì €ì¥ëœ íŒŒì¼ë“¤ë„ í¬í•¨í•©ë‹ˆë‹¤ (collected_dataê°€ ì œê³µëœ ê²½ìš°).
    
    Args:
        results_dir: ì••ì¶•í•  ë””ë ‰í† ë¦¬
        output_path: ì¶œë ¥ ZIP íŒŒì¼ ê²½ë¡œ
        collected_data: ìˆ˜ì§‘ëœ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ (ì˜êµ¬ ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ í¬í•¨)
    """
    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ë“¤ ì¶”ê°€
        for root, dirs, files in os.walk(results_dir):
            for file in files:
                file_path = Path(root) / file
                # results_dir ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ ê²½ë¡œ ìƒì„±
                arcname = file_path.relative_to(results_dir)
                zipf.write(file_path, arcname)
        
        # ì˜êµ¬ ì €ì¥ëœ íŒŒì¼ë“¤ë„ ZIPì— ì¶”ê°€ (ì´ì¤‘ ì €ì¥ ìµœì í™”: ë³µì‚¬ ì—†ì´ ZIPì— ì§ì ‘ ì¶”ê°€)
        if collected_data:
            for item in collected_data:
                if item.get('success'):
                    # ì˜êµ¬ ì €ì¥ëœ ì‘ë‹µ íŒŒì¼ ì¶”ê°€
                    if 'response_path' in item:
                        response_path = Path(item['response_path'])
                        if response_path.exists():
                            zipf.write(response_path, f"data/responses/{response_path.name}")
                    
                    # íŒŒì‹±ëœ ì‘ë‹µ íŒŒì¼ ì¶”ê°€ (ì„ì‹œ ë””ë ‰í† ë¦¬ì— ìˆëŠ” ê²ƒ)
                    if 'parsed_path' in item:
                        parsed_path = Path(item['parsed_path'])
                        if parsed_path.exists():
                            zipf.write(parsed_path, f"data/parsed_responses/{parsed_path.name}")


def main():
    st.title("ğŸ“Š RAGAS í‰ê°€ ì‹œìŠ¤í…œ")
    st.markdown("---")
    
    # ì‚¬ì´ë“œë°”: ì„¤ì •
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        
        # íŒŒì¼ ì—…ë¡œë“œ
        uploaded_file = st.file_uploader(
            "ì¿¼ë¦¬ì…‹ JSON íŒŒì¼ ì—…ë¡œë“œ",
            type=['json'],
            help="data.json í˜•ì‹ì˜ ì¿¼ë¦¬ì…‹ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”"
        )
        
        # URL ë° API í‚¤ ì…ë ¥
        langflow_url = st.text_input(
            "Langflow URL",
            value=os.getenv('LANGFLOW_URL', ''),
            help="Langflow API URL (ì˜ˆ: http://10.1.1.70:7860/api/v1/run/...)",
            type="default"
        )
        
        langflow_api_key = st.text_input(
            "Langflow API Key",
            value=os.getenv('LANGFLOW_API_KEY', ''),
            type="password",
            help="Langflow API Key"
        )
        
        openai_api_key = st.text_input(
            "OpenAI API Key",
            value=os.getenv('OPENAI_API_KEY', ''),
            type="password",
            help="OpenAI API Key (RAGAS í‰ê°€ì— ì‚¬ìš©)"
        )
        
        st.markdown("---")
        
        # í‰ê°€ ì‹œì‘ ë²„íŠ¼
        start_button = st.button(
            "ğŸš€ í‰ê°€ ì‹œì‘",
            type="primary",
            use_container_width=True,
            disabled=st.session_state.is_running
        )
        
        # í‰ê°€ ì¤‘ì§€ ë²„íŠ¼
        if st.session_state.is_running:
            stop_button = st.button(
                "â¹ï¸ í‰ê°€ ì¤‘ì§€",
                type="secondary",
                use_container_width=True
            )
            if stop_button:
                st.session_state.is_running = False
                st.rerun()
    
    # ë©”ì¸ ì˜ì—­
    if uploaded_file is None:
        st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ ì¿¼ë¦¬ì…‹ JSON íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì„¤ì •ì„ ì…ë ¥í•œ í›„ í‰ê°€ë¥¼ ì‹œì‘í•˜ì„¸ìš”.")
        
        # ì˜ˆì‹œ ë°ì´í„° í‘œì‹œ
        with st.expander("ğŸ“ ì¿¼ë¦¬ì…‹ íŒŒì¼ í˜•ì‹ ì˜ˆì‹œ"):
            example_data = [
                {
                    "query": "ê³„ì•½ê·œì •ì˜ ì œì • ëª©ì ì€ ë¬´ì—‡ì¸ê°€?",
                    "ground_truth": "ì´ ê·œì •ì€ íšŒì‚¬ì˜ ê³„ì•½ì—…ë¬´ ì²˜ë¦¬ì— í•„ìš”í•œ ê¸°ë³¸ì‚¬í•­ì„ ì •í•˜ì—¬ ê³„ì•½ì—…ë¬´ì˜ ì›í™œí•œ ìˆ˜í–‰ì„ ë„ëª¨í•˜ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•œë‹¤.",
                    "gt_context": "ê³„ì•½ê·œì • ì œ1ì¡°(ëª©ì )"
                }
            ]
            st.json(example_data)
    else:
        # ì—…ë¡œë“œëœ íŒŒì¼ ì½ê¸°
        try:
            file_content = uploaded_file.read()
            queries_data = json.loads(file_content.decode('utf-8'))
            
            if not isinstance(queries_data, list):
                st.error("âŒ JSON íŒŒì¼ì€ ë°°ì—´ í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
                st.stop()
            
            st.success(f"âœ… {len(queries_data)}ê°œì˜ ì¿¼ë¦¬ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
            
            # ì¿¼ë¦¬ ëª©ë¡ í‘œì‹œ
            with st.expander(f"ğŸ“‹ ì¿¼ë¦¬ ëª©ë¡ ({len(queries_data)}ê°œ)"):
                for i, query_data in enumerate(queries_data, 1):
                    st.markdown(f"**{i}. {query_data.get('query', 'N/A')}**")
            
            # í‰ê°€ ì‹œì‘
            if start_button:
                # ì…ë ¥ ê²€ì¦
                if not langflow_url:
                    st.error("âŒ Langflow URLì„ ì…ë ¥í•˜ì„¸ìš”.")
                    st.stop()
                if not langflow_api_key:
                    st.error("âŒ Langflow API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
                    st.stop()
                if not openai_api_key:
                    st.error("âŒ OpenAI API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
                    st.stop()
                
                # í‰ê°€ ì‹œì‘
                st.session_state.is_running = True
                st.session_state.evaluation_results = []
                st.session_state.progress = 0
                st.session_state.total_queries = len(queries_data)
                
                # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
                temp_dir = Path(tempfile.mkdtemp(prefix="ragas_eval_"))
                zip_path = None  # ZIP íŒŒì¼ ê²½ë¡œ ì¶”ì ìš©
                
                try:
                    # ì§„í–‰ë„ í‘œì‹œ ì˜ì—­
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    results_container = st.container()
                    
                    # í™˜ê²½ë³€ìˆ˜ ì„¤ì •
                    os.environ['LANGFLOW_URL'] = langflow_url
                    os.environ['LANGFLOW_API_KEY'] = langflow_api_key
                    os.environ['OPENAI_API_KEY'] = openai_api_key
                    
                    # 1ë‹¨ê³„: ëª¨ë“  ì¿¼ë¦¬ì— ëŒ€í•´ ë°ì´í„° ìˆ˜ì§‘
                    status_text.text("ğŸ“¥ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
                    collected_data = []
                
                for idx, query_data in enumerate(queries_data):
                    if not st.session_state.is_running:
                        st.warning("âš ï¸ í‰ê°€ê°€ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                        break
                    
                    # ì§„í–‰ë„ ì—…ë°ì´íŠ¸ (ë°ì´í„° ìˆ˜ì§‘ ë‹¨ê³„: 0~70%)
                    progress = (idx + 1) / len(queries_data) * 0.7
                    progress_bar.progress(progress)
                    status_text.text(f"ë°ì´í„° ìˆ˜ì§‘ ì¤‘: {idx + 1}/{len(queries_data)} - {query_data.get('query', 'N/A')[:50]}...")
                    
                    # ë°ì´í„° ìˆ˜ì§‘
                    result = collect_query_data(
                        query_data,
                        langflow_url,
                        langflow_api_key,
                        temp_dir
                    )
                    
                    collected_data.append(result)
                    
                    # ê²°ê³¼ í‘œì‹œ
                    with results_container:
                        if result['success']:
                            st.success(f"âœ… {result['query'][:50]}... (ìˆ˜ì§‘ ì™„ë£Œ)")
                        else:
                            st.error(f"âŒ {result['query'][:50]}... - {result.get('error', 'Unknown error')}")
                
                # 2ë‹¨ê³„: ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ í›„ í‰ê°€ ì‹¤í–‰
                if st.session_state.is_running:
                    status_text.text("ğŸ“Š í‰ê°€ ì‹¤í–‰ ì¤‘...")
                    progress_bar.progress(0.75)
                    
                    # ì„±ê³µí•œ ë°ì´í„°ë§Œ í•„í„°ë§
                    successful_data = [item for item in collected_data if item['success']]
                    
                    if len(successful_data) == 0:
                        st.error("âŒ í‰ê°€í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        st.session_state.is_running = False
                    else:
                        try:
                            # ë°°ì¹˜ í‰ê°€ ë°ì´í„° ì¤€ë¹„ (ë‹µë³€ì´ ì—†ê±°ë‚˜ contextê°€ nullì¸ ì¼€ì´ìŠ¤ ì œì™¸)
                            progress_bar.progress(0.8)
                            dataset, included_items = prepare_batch_evaluation_data(successful_data)
                            
                            # ì œì™¸ëœ ì¼€ì´ìŠ¤ í™•ì¸
                            excluded_items = [item for item in successful_data if item not in included_items]
                            if excluded_items:
                                excluded_queries = [item['query'] for item in excluded_items]
                                st.warning(f"âš ï¸ ë‹¤ìŒ {len(excluded_items)}ê°œ ì¿¼ë¦¬ëŠ” ë‹µë³€ì´ ì—†ê±°ë‚˜ contextê°€ nullì´ì–´ì„œ í‰ê°€ì—ì„œ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤:")
                                for query in excluded_queries:
                                    st.caption(f"  - {query[:60]}...")
                            
                            # RAGAS í‰ê°€ ì‹¤í–‰ (ì „ì²´ DataFrame ì–»ê¸°)
                            from ragas import evaluate
                            from ragas.metrics import (
                                faithfulness,
                                answer_relevancy,
                                context_precision,
                                context_recall,
                            )
                            from ragas.llms import llm_factory
                            from openai import OpenAI
                            
                            # OpenAI í´ë¼ì´ì–¸íŠ¸ ë° LLM ì„¤ì •
                            api_key = os.getenv('OPENAI_API_KEY')
                            openai_client = OpenAI(api_key=api_key)
                            llm = llm_factory("gpt-4o-mini", client=openai_client)
                            
                            # í‰ê°€ ë©”íŠ¸ë¦­ ì„¤ì •
                            metrics = [
                                faithfulness.__class__(llm=llm),
                                answer_relevancy.__class__(llm=llm),
                                context_precision.__class__(llm=llm),
                                context_recall.__class__(llm=llm),
                            ]
                            
                            # ì „ì²´ í‰ê°€ ì‹¤í–‰
                            progress_bar.progress(0.9)
                            full_result = evaluate(dataset=dataset, metrics=metrics)
                            results_df = full_result.to_pandas()
                            
                            # ê° ì¿¼ë¦¬ë³„ë¡œ ê²°ê³¼ ì €ì¥
                            results_dir = temp_dir / "results"
                            results_dir.mkdir(parents=True, exist_ok=True)
                            
                            evaluation_results_list = []
                            
                            # í‰ê°€ì— í¬í•¨ëœ í•­ëª©ë“¤ì— ëŒ€í•´ ê²°ê³¼ ë§¤í•‘
                            for idx, item in enumerate(included_items):
                                if idx < len(results_df):
                                    row = results_df.iloc[idx]
                                    result_dict = {
                                        'faithfulness': float(row.get('faithfulness', 0)) if pd.notna(row.get('faithfulness', 0)) else 0,
                                        'answer_relevancy': float(row.get('answer_relevancy', 0)) if pd.notna(row.get('answer_relevancy', 0)) else 0,
                                        'context_precision': float(row.get('context_precision', 0)) if pd.notna(row.get('context_precision', 0)) else 0,
                                        'context_recall': float(row.get('context_recall', 0)) if pd.notna(row.get('context_recall', 0)) else 0,
                                    }
                                else:
                                    # ê¸°ë³¸ê°’ ì‚¬ìš©
                                    result_dict = {
                                        'faithfulness': 0,
                                        'answer_relevancy': 0,
                                        'context_precision': 0,
                                        'context_recall': 0,
                                    }
                                
                                # ê°œë³„ ê²°ê³¼ ì €ì¥
                                json_path = save_evaluation_results(
                                    result_dict,
                                    [item['query_data']],
                                    item['parsed_data'],
                                    str(results_dir)
                                )
                                
                                evaluation_results_list.append({
                                    'query': item['query'],
                                    'success': True,
                                    'results': result_dict,
                                    'json_path': json_path,
                                    'parsed_path': item.get('parsed_path')
                                })
                            
                            # í‰ê°€ì—ì„œ ì œì™¸ëœ í•­ëª©ë“¤ (ë‹µë³€ì´ ì—†ê±°ë‚˜ contextê°€ null)
                            for item in excluded_items:
                                evaluation_results_list.append({
                                    'query': item['query'],
                                    'success': False,
                                    'error': 'ë‹µë³€ì´ ì—†ê±°ë‚˜ contextê°€ nullì´ì–´ì„œ í‰ê°€ì—ì„œ ì œì™¸ë¨',
                                    'error_type': 'ExcludedFromEvaluation'
                                })
                            
                            # ì‹¤íŒ¨í•œ ì¿¼ë¦¬ë„ ê²°ê³¼ì— í¬í•¨
                            for item in collected_data:
                                if not item['success']:
                                    evaluation_results_list.append(item)
                            
                            st.session_state.evaluation_results = evaluation_results_list
                            
                            # í‰ê·  ì ìˆ˜ ê³„ì‚° ë° ì €ì¥
                            successful_eval_results = [r for r in evaluation_results_list if r['success']]
                            if successful_eval_results:
                                metrics = ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']
                                avg_scores = {}
                                for metric in metrics:
                                    scores = []
                                    for r in successful_eval_results:
                                        if 'results' in r and metric in r['results']:
                                            score = r['results'][metric]
                                            if isinstance(score, (int, float)) and not (isinstance(score, float) and score != score):  # NaN ì²´í¬
                                                scores.append(score)
                                    if scores:
                                        avg_scores[metric] = sum(scores) / len(scores)
                                
                                # í‰ê·  ì ìˆ˜ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
                                if avg_scores:
                                    avg_results_data = {
                                        "timestamp": datetime.now().isoformat(),
                                        "summary": {
                                            "total_queries": len(evaluation_results_list),
                                            "successful_queries": len(successful_eval_results),
                                            "failed_queries": len(evaluation_results_list) - len(successful_eval_results)
                                        },
                                        "average_scores": avg_scores
                                    }
                                    
                                    avg_json_filepath = results_dir / "average_scores.json"
                                    with open(avg_json_filepath, 'w', encoding='utf-8') as f:
                                        json.dump(avg_results_data, f, ensure_ascii=False, indent=2)
                                    
                                    print(f"í‰ê·  ì ìˆ˜ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {avg_json_filepath}")
                            
                            progress_bar.progress(1.0)
                            status_text.text("âœ… ëª¨ë“  í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                            
                        except ValueError as e:
                            # í‰ê°€í•  ìˆ˜ ìˆëŠ” ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°
                            st.error(f"âŒ {str(e)}")
                            st.session_state.evaluation_results = collected_data
                            st.session_state.is_running = False
                        except Exception as e:
                            st.error(f"âŒ í‰ê°€ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                            import traceback
                            st.code(traceback.format_exc())
                            st.session_state.evaluation_results = collected_data
                
                # í‰ê°€ ì™„ë£Œ
                if st.session_state.is_running:
                    st.session_state.is_running = False
                    
                    # ê²°ê³¼ ìš”ì•½
                    st.markdown("---")
                    st.header("ğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½")
                    
                    successful = sum(1 for r in st.session_state.evaluation_results if r['success'])
                    failed = len(st.session_state.evaluation_results) - successful
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("ì „ì²´ ì¿¼ë¦¬", len(st.session_state.evaluation_results))
                    with col2:
                        st.metric("ì„±ê³µ", successful, delta=f"{successful/len(st.session_state.evaluation_results)*100:.1f}%")
                    with col3:
                        st.metric("ì‹¤íŒ¨", failed, delta=f"-{failed/len(st.session_state.evaluation_results)*100:.1f}%")
                    
                    # ì„±ê³µí•œ í‰ê°€ì˜ í‰ê·  ì ìˆ˜ ê³„ì‚°
                    if successful > 0:
                        successful_results = [r for r in st.session_state.evaluation_results if r['success']]
                        metrics = ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']
                        
                        st.markdown("### ğŸ“ˆ í‰ê·  í‰ê°€ ì ìˆ˜")
                        avg_scores = {}
                        for metric in metrics:
                            scores = []
                            for r in successful_results:
                                if 'results' in r and metric in r['results']:
                                    score = r['results'][metric]
                                    if isinstance(score, (int, float)) and not (isinstance(score, float) and score != score):  # NaN ì²´í¬
                                        scores.append(score)
                            if scores:
                                avg_scores[metric] = sum(scores) / len(scores)
                        
                        if avg_scores:
                            cols = st.columns(len(avg_scores))
                            for i, (metric, score) in enumerate(avg_scores.items()):
                                with cols[i]:
                                    st.metric(metric.replace('_', ' ').title(), f"{score:.4f}")
                    
                    # ì••ì¶• íŒŒì¼ ìƒì„± ë° ë‹¤ìš´ë¡œë“œ
                    st.markdown("---")
                    st.header("ğŸ“¦ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ")
                    
                    zip_filename = f"ragas_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}.zip"
                    zip_path = temp_dir.parent / zip_filename
                    
                    try:
                        create_zip_file(temp_dir, zip_path, collected_data)
                        
                        # ZIP íŒŒì¼ ì½ê¸°
                        zip_data = None
                        with open(zip_path, 'rb') as f:
                            zip_data = f.read()
                        
                        if zip_data:
                            st.download_button(
                                label="ğŸ“¥ ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ (ZIP)",
                                data=zip_data,
                                file_name=zip_filename,
                                mime="application/zip",
                                use_container_width=True
                            )
                            
                            st.success(f"âœ… ì••ì¶• íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {zip_filename}")
                            
                            # íŒŒì¼ í¬ê¸° í‘œì‹œ
                            file_size = len(zip_data) / (1024 * 1024)  # MB
                            st.caption(f"íŒŒì¼ í¬ê¸°: {file_size:.2f} MB")
                        else:
                            st.error("âŒ ì••ì¶• íŒŒì¼ ìƒì„± ì‹¤íŒ¨")
                        
                    except Exception as e:
                        st.error(f"âŒ ì••ì¶• íŒŒì¼ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                        import traceback
                        st.code(traceback.format_exc())
                    
                    # ìƒì„¸ ê²°ê³¼ í‘œì‹œ
                    with st.expander("ğŸ“‹ ìƒì„¸ ê²°ê³¼"):
                        for i, result in enumerate(st.session_state.evaluation_results, 1):
                            if result['success']:
                                st.markdown(f"#### {i}. {result['query']}")
                                st.json(result['results'])
                            else:
                                st.markdown(f"#### {i}. {result['query']} âŒ")
                                st.error(f"ì˜¤ë¥˜: {result.get('error', 'Unknown error')}")
                
                finally:
                    # ì„ì‹œ ë””ë ‰í† ë¦¬ ë° ZIP íŒŒì¼ ì •ë¦¬
                    try:
                        if temp_dir.exists():
                            shutil.rmtree(temp_dir)
                            print(f"ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì™„ë£Œ: {temp_dir}")
                    except Exception as e:
                        print(f"ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ ê°€ëŠ¥): {e}")
                    
                    try:
                        if zip_path and zip_path.exists():
                            # ZIP íŒŒì¼ì€ ë‹¤ìš´ë¡œë“œ í›„ ì¼ì • ì‹œê°„ í›„ ì‚­ì œí•˜ëŠ” ê²ƒì´ ì¢‹ì§€ë§Œ,
                            # Streamlitì—ì„œëŠ” ì„¸ì…˜ë³„ë¡œ ê´€ë¦¬í•˜ê¸° ì–´ë ¤ìš°ë¯€ë¡œ 
                            # ìµœì†Œí•œ ì„¸ì…˜ ìƒíƒœì— ê²½ë¡œë¥¼ ì €ì¥í•˜ê³  ë‹¤ìŒ í˜ì´ì§€ ë¡œë“œ ì‹œ ì •ë¦¬í•  ìˆ˜ ìˆë„ë¡ í•¨
                            # ì—¬ê¸°ì„œëŠ” ì¦‰ì‹œ ì‚­ì œí•˜ì§€ ì•Šê³  ë‚¨ê²¨ë‘  (ì‚¬ìš©ìê°€ ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆë„ë¡)
                            # ëŒ€ì‹  ZIP íŒŒì¼ ê²½ë¡œë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                            if 'zip_files_to_cleanup' not in st.session_state:
                                st.session_state.zip_files_to_cleanup = []
                            st.session_state.zip_files_to_cleanup.append(str(zip_path))
                    except Exception as e:
                        print(f"ZIP íŒŒì¼ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ ê°€ëŠ¥): {e}")
                
        except json.JSONDecodeError:
            st.error("âŒ JSON íŒŒì¼ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        except Exception as e:
            st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            st.code(traceback.format_exc())


if __name__ == '__main__':
    main()

